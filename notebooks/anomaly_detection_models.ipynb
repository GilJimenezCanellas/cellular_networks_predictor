{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/gil/Telecos/3B/APA/LAB/Competition/data/ML-MATT-CompetitionQT2021_train.csv', delimiter=';')\n",
    "test = pd.read_csv('/home/gil/Telecos/3B/APA/LAB/Competition/data/ML-MATT-CompetitionQT2021_test.xls', delimiter=';')\n",
    "\n",
    "train_ = pd.read_csv('/home/gil/Telecos/3B/APA/LAB/Competition/ML-MATT-CompetitionQT2021_train.csv', delimiter=';')\n",
    "test_ = pd.read_csv('/home/gil/Telecos/3B/APA/LAB/Competition/data/ML-MATT-CompetitionQT2021_test.csv', delimiter=';')\n",
    "test_ = test_.drop(columns='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with at least one difference between test and test_: 18316\n"
     ]
    }
   ],
   "source": [
    "concatenated_test = pd.concat([test, test_], keys=['test', 'test_'])\n",
    "\n",
    "different_rows = concatenated_test.reset_index().drop_duplicates(keep=False)\n",
    "\n",
    "different_rows_count = len(different_rows)\n",
    "\n",
    "print(\"Number of rows with at least one difference between test and test_:\", different_rows_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 1s in balanced data: 27.593214827660955\n",
      "Percentage of 0s in balanced data: 72.40678517233904\n",
      "   Time  PRBUsageUL  PRBUsageDL  meanThr_DL  meanThr_UL  maxThr_DL  maxThr_UL  \\\n",
      "0   645     12.3848      1.4019      0.3927      0.0438    16.6522     0.6806   \n",
      "1   585     22.0438      2.0016      0.5620      0.2697    10.3994     1.1771   \n",
      "2   465      0.5105      0.4258      0.0152      0.0106     0.2755     0.1685   \n",
      "3   165      1.9963      1.1513      0.9908      0.0245    64.7465     0.8747   \n",
      "4   210      0.3030      0.4040      0.0160      0.0130     0.3480     0.1680   \n",
      "\n",
      "   meanUE_DL  meanUE_UL  maxUE_DL  maxUE_UL  maxUE_UL+DL  \n",
      "0     1.1293     1.0491         5         3            8  \n",
      "1     1.4480     1.1630         6         5           11  \n",
      "2     1.0379     1.0535         1         2            3  \n",
      "3     1.0766     1.0526         3         2            5  \n",
      "4     1.0110     1.0110         2         1            3  \n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Unusual, dtype: int64\n",
      "   Time  PRBUsageUL  PRBUsageDL  meanThr_DL  meanThr_UL  maxThr_DL  maxThr_UL  \\\n",
      "4   210       0.303       0.404       0.016       0.013      0.348      0.168   \n",
      "5   810      15.966       1.819       0.415       0.071     10.116      0.706   \n",
      "6  1200       7.074       0.505       0.032       0.012      1.680      0.131   \n",
      "8  1005      26.879       3.032       0.525       0.120      9.145      0.894   \n",
      "9   255       4.143       0.505       0.021       0.013      0.409      0.437   \n",
      "\n",
      "   meanUE_DL  meanUE_UL  maxUE_DL  maxUE_UL  maxUE_UL+DL  \n",
      "4      1.011      1.011         2         1            3  \n",
      "5      1.364      1.314         6         5           11  \n",
      "6      1.041      1.041         3         3            6  \n",
      "8      1.425      1.273         6         5           11  \n",
      "9      1.021      0.010         2         2            4  \n",
      "   Time  PRBUsageUL  PRBUsageDL  meanThr_DL  meanThr_UL  maxThr_DL  maxThr_UL  \\\n",
      "0   180      38.177     15.2510      0.5829      0.0445   231688.0     0.7669   \n",
      "1  1230       2.021      3.3350      0.5690      0.0750    29265.0     1.0490   \n",
      "2   690       0.505      0.4040      0.0140      0.0100      227.0     0.0970   \n",
      "3   405       1.011      0.5050      0.2380      0.0210    20962.0     0.6090   \n",
      "4   945      40.269      0.5104      0.0807      0.0414    39317.0    17.8110   \n",
      "\n",
      "   meanUE_DL  meanUE_UL  maxUE_DL  maxUE_UL  maxUE_UL+DL  \n",
      "0     10.262     0.0100         3         3            6  \n",
      "1      1.314     0.0100         6         3            9  \n",
      "2      1.011     0.0100         2         1            3  \n",
      "3      1.011     1.0110         2         1            3  \n",
      "4     10.575     0.0107         3         2            5  \n"
     ]
    }
   ],
   "source": [
    "path = '/home/gil/Telecos/3B/APA/LAB/Competition/data/'\n",
    "train = pd.read_csv(path + 'ML-MATT-CompetitionQT2021_train.csv', delimiter=';')\n",
    "test = pd.read_csv(path + 'ML-MATT-CompetitionQT2021_test.csv', delimiter=';')\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# file_path_train = '/content/drive/My Drive/APA/Competition/data/ML-MATT-CompetitionQT2021_train.csv'\n",
    "# file_path_test = '/content/drive/My Drive/APA/Competition/data/ML-MATT-CompetitionQT2021_test.csv'\n",
    "# train = pd.read_csv(file_path_train, delimiter=';')\n",
    "# test = pd.read_csv(file_path_test, delimiter=';')\n",
    "\n",
    "# Drop CellName and prepare Time\n",
    "train = train.drop(columns=['CellName'])\n",
    "time_parts = train['Time'].str.split(':', expand=True).astype(int)\n",
    "seconds_since_midnight = (time_parts[0] * 60) + time_parts[1]\n",
    "train['Time'] = seconds_since_midnight\n",
    "\n",
    "# Drop CellName and prepare Time\n",
    "test = test.drop(columns=['CellName'])\n",
    "time_parts = test['Time'].str.split(':', expand=True).astype(int)\n",
    "seconds_since_midnight = (time_parts[0] * 60) + time_parts[1]\n",
    "test['Time'] = seconds_since_midnight\n",
    "test = test.drop(columns=['ID'])\n",
    "\n",
    "# Remove all non-numeric characters\n",
    "test['maxThr_DL'] = test['maxThr_DL'].replace('[^\\d]', '', regex=True)\n",
    "test['maxThr_DL'] = test['maxThr_DL'].str.replace(r'(\\d+\\.\\d+)(.*)', r'\\1', regex=True).astype(float)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "normal = train[train['Unusual'] == 0]\n",
    "normal = normal.drop('Unusual', axis=1)\n",
    "abnormal = train[train['Unusual'] == 1]\n",
    "\n",
    "# Undersample majority class to match the desired ratio\n",
    "# from sklearn.utils import resample\n",
    "# desired_normal_count = int(len(abnormal) / 0.5 * 0.5)  # Calculate the desired number of samples for the majority class\n",
    "# undersampled_normal = resample(normal, replace=False, n_samples=desired_normal_count, random_state=42)\n",
    "\n",
    "# # Combine minority class with undersampled majority class\n",
    "# train = pd.concat([undersampled_normal, abnormal])\n",
    "\n",
    "# # Shuffle the dataframe\n",
    "# train = train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Percentage of 1s in balanced data:\", (train['Unusual'] == 1).mean() * 100)\n",
    "print(\"Percentage of 0s in balanced data:\", (train['Unusual'] == 0).mean() * 100)\n",
    "\n",
    "X = train.drop('Unusual', axis=1)\n",
    "y = train['Unusual']\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())\n",
    "print(normal.head())\n",
    "print(test.head())\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['meanUE_UL', 'PRBUsageUL', 'PRBUsageDL']\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "normal = normal[selected_features]\n",
    "abnormal = abnormal[selected_features]\n",
    "test = test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator IsolationForest(random_state=42) does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(isolationf, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fit the grid search to the data\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get the best estimator from the grid search\u001b[39;00m\n\u001b[1;32m     20\u001b[0m best_isf \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:800\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    798\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 800\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m _check_multimetric_scoring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_scorer.py:953\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf no scoring is specified, the estimator passed should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method. The estimator \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m estimator\n\u001b[1;32m    956\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator IsolationForest(random_state=42) does not."
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_samples': [0.5, 0.7, 0.9],\n",
    "    'contamination': [0.05, 0.1, 0.15],\n",
    "    'max_features': [0.5, 0.7, 0.9],\n",
    "    'bootstrap': [True, False],\n",
    "    'random_state': [42] \n",
    "}\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "isolationf = IsolationForest(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(isolationf, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(normal)\n",
    "\n",
    "# Get the best estimator from the grid search\n",
    "best_isf = grid_search.best_estimator_\n",
    "\n",
    "print(best_isf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15473191795609934\n"
     ]
    }
   ],
   "source": [
    "# isf = best_isf\n",
    "isf = IsolationForest(n_estimators=100, max_samples=0.7, max_features=0.9, contamination=0.1, bootstrap=True, random_state=46)\n",
    "\n",
    "isf.fit(normal)\n",
    "\n",
    "y_pred = isf.predict(X_test)\n",
    "\n",
    "y_p = [1 if x == -1 else 0 for x in y_pred]\n",
    "\n",
    "f1 = f1_score(y_test, y_p)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
